services:
  # --- EXISTANT ---

  jupyter:
    build: ./jupyter
    ports:
      - "8888:8888"
    volumes:
      - ./jupyter/notebooks:/notebooks

  mlflow:
    build: ./mlflow
    image: mlflow:local          # tag local pour Airflow
    container_name: mlflow
    ports:
      - "5000:5000"              # UI tracking
      - "5001:5001"              # serving modèle
    volumes:
      - ./datas:/app/datas
      - ./mlruns:/app/mlruns

  ray:
    build: ./ray
    container_name: ray
    ports:
      - "6380:6380"
      - "8265:8265"
      - "10001:10001"
  redis:
    build: ./redis
    container_name: redis
    ports:
      - "6379:6379"

  postgres:
    build: ./postgres
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U airflow"]
      interval: 5s
      timeout: 3s
      retries: 20

  airflow_init:
    build: ./airflow_init
    container_name: airflow_init
    depends_on:
      - postgres
    environment:
      # Airflow 3.x : Execution API + secret JWT partagé
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow_api-server:8080/execution/"
      AIRFLOW__API_AUTH__JWT_SECRET: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWUsImlhdCI6MTUxNjIzOTAyMn0.KMUFsIDTnFmyG3nMiGM6H9FNFUROf3wh7SmqJp-QV30"
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "True"

      # DB
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      DB_HOST: postgres
      DB_PORT: 5432

      # Admin UI
      ADMIN_USERNAME: admin
      ADMIN_PASSWORD: admin
      ADMIN_EMAIL: admin@example.com
    volumes:
      - ./dags:/opt/airflow/dags
      - ./init-data:/init-data

  airflow_api-server:
    build: ./airflow_api-server
    container_name: airflow_api-server
    ports:
      - "8080:8080"
    depends_on:
      - postgres
      - airflow_init
    environment:
      # Executor = Celery
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor

      # Execution API + secret JWT
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow_api-server:8080/execution/"
      AIRFLOW__API_AUTH__JWT_SECRET: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWUsImlhdCI6MTUxNjIzOTAyMn0.KMUFsIDTnFmyG3nMiGM6H9FNFUROf3wh7SmqJp-QV30"
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "True"

      # Celery broker/backend
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow

      # Divers
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      DB_HOST: postgres
      DB_PORT: 5432

      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./init-data:/init-data

  airflow-scheduler:
    build: ./airflow_scheduler
    container_name: airflow-scheduler
    depends_on:
      - postgres
      - redis
      - airflow_init
      - airflow_api-server
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor

      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow_api-server:8080/execution/"
      AIRFLOW__API_AUTH__JWT_SECRET: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWUsImlhdCI6MTUxNjIzOTAyMn0.KMUFsIDTnFmyG3nMiGM6H9FNFUROf3wh7SmqJp-QV30"
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "True"

      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow

      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      DB_HOST: postgres
      DB_PORT: 5432
    volumes:
      - ./dags:/opt/airflow/dags
      - ./init-data:/init-data

  airflow-triggerer:
    build: ./airflow_triggerer
    container_name: airflow-triggerer
    depends_on:
      - postgres
      - airflow_init
      - airflow_api-server
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor

      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow_api-server:8080/execution/"
      AIRFLOW__API_AUTH__JWT_SECRET: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWUsImlhdCI6MTUxNjIzOTAyMn0.KMUFsIDTnFmyG3nMiGM6H9FNFUROf3wh7SmqJp-QV30"
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "True"

      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow

      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      DB_HOST: postgres
      DB_PORT: 5432
    volumes:
      - ./dags:/opt/airflow/dags
      - ./init-data:/init-data

  airflow_dag-processor:
    build: ./airflow_dag-processor
    container_name: airflow_dag-processor
    depends_on:
      - postgres
      - airflow_init
      - airflow_api-server
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor

      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow_api-server:8080/execution/"
      AIRFLOW__API_AUTH__JWT_SECRET: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWUsImlhdCI6MTUxNjIzOTAyMn0.KMUFsIDTnFmyG3nMiGM6H9FNFUROf3wh7SmqJp-QV30"
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "True"

      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow

      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      DB_HOST: postgres
      DB_PORT: 5432
    volumes:
      - ./dags:/opt/airflow/dags
      - ./init-data:/init-data

  # NOUVEAU : worker Celery
  airflow-worker:
    build: ./airflow_worker           # assure-toi d'avoir ce dossier/Dockerfile
    container_name: airflow-worker
    depends_on:
      - redis
      - postgres
      - airflow_api-server
      - airflow_init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor

      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow_api-server:8080/execution/"
      AIRFLOW__API_AUTH__JWT_SECRET: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWUsImlhdCI6MTUxNjIzOTAyMn0.KMUFsIDTnFmyG3nMiGM6H9FNFUROf3wh7SmqJp-QV30"

      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow

      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      DB_HOST: postgres
      DB_PORT: 5432
    volumes:
      - ./dags:/opt/airflow/dags
      - ./init-data:/init-data

  prometheus:
    build: ./prometheus
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/yaml/prometheus.yml:/etc/prometheus/prometheus.yml:ro


  statsd-exporter:
    build: ./statsd-exporter
    container_name: statsd-exporter
    command: >
      --statsd.listen-udp=:9125
      --web.listen-address=:9102
    ports:
      - "9102:9102"            # Expose si Prometheus est hors Docker


  grafana:
    build: ./grafana
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin   # change-le après le 1er login
      GF_USERS_DEFAULT_THEME: dark
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/provisioning/dashboards/json:/var/lib/grafana/dashboards
    depends_on:
      - prometheus
